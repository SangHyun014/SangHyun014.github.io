---
layout: post
title: "[정보이론] Entorypy와 KL Divergence"
categories: [Theory]
tags: [정보이론, Entropy]
---


# 정보 이론이란?

우선 정보 이론을 살펴보기에 앞서 정보에 대해 살펴보겠다.
> 정보 : 컴퓨터 공학에서 특정 목적을 위하여 광(光) 또는 전자적 방식으로 처리되어 부호, 문자, 음성, 음향 및 영상 등을 표현하는 모든 종류의 자료 또는 지식을 의미한다.

이러한 정보를 주고 받을 때, "정보"라는 것이 정량화 된 정보의 양을 정보량이라 할 수 있다.

### 정보량
정보 보론에서 정보량은 '놀람의 정도'를 의미한다. 

$\to$ 놀람이 클 때, 정보가 크다고 하는데, 일상적으로 발생하는 사건에 대한 정보는 정보량이 작다고 표현을 하고 자주 발생하지 않는 사건에 대한 정보량은 크다고 표현한다.

그래서 정보 이론이란 이러한 정보를 수치적, 확률적으로 표현한 이론을 정보 이론이라 한다.
이 때 사용하는 단위가 '엔트로피(Entropy)'를 사용한다.
이는 머신러닝에서 해당 확률분포의 특성을 알아내거나 확률분포 간의 유사성을 정량화하는데 쓰인다.

>정보이론의 핵심은 잘 일어나지 않는 사건(Unlikely event)의 정보는 자주 발생하는 사건보다 정보량이 많다는 것이다. 

- 자주 발생하는 사건은 낮은 정보량을 가진다.
- 독립사건(Independent event)는 추가적인 정보량(additive information)을 가진다. 예를들어, 동전을 던져 앞면이 두번 나오는 사건에 대한 정보량은 동전을 던져 앞면이 한번 나오는 정보량의 두 배이다.

    -> 이에 대해 나는 사건의 확률과 정보량은 반비례하는 관계로 이해했다.


## 섀넌 엔트로피
우선 확률변수 X의 값이 $x$인 사건의 정보량은 다음과 같다.
$$
I(x) = -logP(x)
$$
이를 바탕으로 앞선 동전의 예시에 적용해보았다.

동전을 한번 던지는 정보량 : $-log_2^{0.5}=1$

동전을 두번 던지는 정보량 : $-log_2^{0.25}=2$

따라서 앞서 말한 것처럼 정보량이 두 배가 되는 것을 확인할 수 있다. 따라서 동전을 두번 던져 앞면이 나오는 사건의 확률이 더 낮아 덜 발생하므로 더 높은 정보량을 가지게 된다는 것을 의미한다.

위 식에서 밑이 2인 경우 정보량의 단위를 섀년(shannon) 또는 비트(bit)라고 한다. 자연상수(e)를 밑으로 할 경우 내트(nat)라고 한다. 머신러닝에서는 주로 자연상수를 사용한다. 

<span style='background-color: #fff5b1'>섀넌 엔트로피(shannon entropy)는 모든 사건 정보량의 기대값을 의미</span>한다.전체 사건 확률분포의 불확실성의 양을 나타낼 때 사용한다. 어떤 확률분포 $P$에 대한 섀넌 엔트로피(shanon entropy) $H(P)$는 다음과 같다.
$$
H(P)=H(x)=E_{X\sim P}[I(X)]=E_{X\sim P}[-logP(x)]
$$
이를 위 사건인 동전을 두번 던지는 사건에 적용해보면,
$$
\begin{matrix}
H(P)=H(x)&=&-\sum_xP(x)logP(x)\\
         &=&-(0.25\times log_20.25+0.5\times log_20.5+0.25\times log_20.25) \\
         &=&-(-1.5)\\
         &=&1.5
\end{matrix}
$$

이렇게 일반적인 동전 던지기의 경우 엔트로피는 높게 나타난다. 이유는 결과값을 예측하기 어렵기 때문이다.

## KL Divergence
쿨백-라이블러 발산(Kullback-Leibler divergence, $D_{KL}$)은 두 확률분포의 차이를 계산하는데 사용하는 함수이다. 여기서 두 확률분포의 차이를 계산한다는 것은 두 확률분포의 <span style='background-color: #fff5b1'>**엔트로피**의 차이</span>를 계산하는 것이다.

딥러닝에서의 상황을 예시로 들어보면, 다음과 같이 표현할 수 있습니다.
- $P(X)$ : 실제 데이터의 분포
- $Q(X)$ : 모델이 추정한 데이터의 분포

$$
D_{KL}(P||Q)=E_{X\sim P}[-logQ(x)]-E_{X\sim P}[-logP(x)]=E_{X\sim P}[-log\frac{Q(x)}{P(x)}]
$$

만약, P와 Q가 동일한 확률분포를 가지게 될 경우 $D_{KL}$의 값은 0이 된다. 그리고 $D_{KL}$은 비대칭(non symmetric)이기에 P와 Q의 위치가 반대가 될 경우 결과값도 달라진다.

## 크로스 엔트로피(Cross Entropy)
먼저 이산확률 변수의 상황에서 위의 $D_{KL}$의 수식을 풀어서 보자.
$$
\begin{matrix}
D_{KL}(P||Q) &=& E_{X\sim P}[-log\frac{Q(x)}{P(x)}]\ldots(1)\\
       &=& -\sum_xP(x)\{ logQ(x) - logP(x) \} \ldots(2)\\
       &=& -\sum_x\{P(x)logQ(x)-P(x)logP(x)\}\ldots(3)\\
       &=& -\sum_xP(x)logQ(x)+ \sum_xP(x)logP(x)\ldots(4)\\
       &=& H(P,Q)-H(P)\ldots(5)
\end{matrix}
$$


4번째 수식에서 볼 수 있듯이, $H(P,Q) = -\sum_xP(x)logQ(x)$로 정의된다.엔트로피이긴하지만 $P(x)$와$Q(x)$가 교차로 곱해졌다는 의미로 크로스 엔트로피(Cross Entropy)로 불린다.

마지막 부분을 $H(P,Q)$에 대해 다시 정리하면 아래의 식과 같다.
$$
H(P,Q)=H(P)+D_{KL}(P||Q)
$$

딥러닝 모델을 학습할 때 이러한 크로스 엔트로피를 최소화하는 방향으로 파라미터(weight)들을 업데이트 한다. 위 식에서 $P(x)$를 가지고 있는 데이터의 분포, $Q(x)$를 모델이 추정하는 데이터의 분포라 하면 크로스 엔트로피$H(P,Q)$를 최소화한다는 것은 $D_{KL}$을 최소화하는 것과 같은 의미로 볼 수 있다. 결과적으로 크로스 엔트로피 최소화는 데이터의 분포와 모델이 추정한 데이터의 분포를 최소화 한다고 이해할 수 있다.
